{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import traceback\n",
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"XML Parse\")\\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.metrics.conf.*.sink.console.class\", \"org.apache.spark.metrics.sink.ConsoleSink\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "logger = logging.getLogger(\"foo\")\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBL_FORMAT = \"delta\"\n",
    "PARTITION = \"partition_key\"\n",
    "TIME_FORMAT = \"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"\n",
    "DELTA_CUR_PATH = \"/user/danielqueiroz/datalake/poc_api/raw/acbsDelta\"\n",
    "URL_API = '_URL_'\n",
    "CSV_PATH = \"/user/danielqueiroz/datalake/poc_api/dados_envio\"\n",
    "DELTA_OK_PATH = \"/user/danielqueiroz/datalake/poc_api/tbl_ctl_envio_xml_OK_100\"\n",
    "DELTA_REPROC_PATH = \"/user/danielqueiroz/datalake/poc_api/tbl_ctl_envio_xml_REPROC_100\"\n",
    "DELTA_DO_NOT_SEND_PATH = \"/user/danielqueiroz/datalake/poc_api/tbl_ctl_envio_xml_DO_NOT_SEND_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(\"string\")\n",
    "def arrears_mex_process_request(row):\n",
    "    '''\n",
    "        Retorno: id enviado pelo response quando o request é bem sucedido\n",
    "    '''\n",
    "    \n",
    "    response_key = \"\"\n",
    "    headers = { \n",
    "        'accept': 'application/json',\n",
    "        'Content-Type': 'application/json; charset=UTF-8'\n",
    "    }\n",
    "    try:\n",
    "        response = requests.post(url=URL_API, headers=headers, data=str(row))\n",
    "        response_key = response.json().get(\"id\")\n",
    "        if response_key is None: \n",
    "            response_key = \"\"\n",
    "        \n",
    "    except: \n",
    "        response_key = \"\"\n",
    "        logger.error(\"erro ao processar o request: %s\".format(status))\n",
    "    return '' #response_key \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_table_exists(delta_path: str) -> bool:\n",
    "    '''\n",
    "        checa se a tabela do (delta lake) existe\n",
    "    '''\n",
    "    ret = False\n",
    "    try:\n",
    "        dataset = spark.read.format(TBL_FORMAT).load(delta_path)\n",
    "        ret = True\n",
    "    except:\n",
    "        ret = False\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_load_data(delta_path: str) -> DataFrame:\n",
    "    '''\n",
    "        carrega os dados que devem ser processados, retorna um dataframe\n",
    "    '''\n",
    "    dataset = spark.read.format(TBL_FORMAT).load(delta_path)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enviroment_if_not_exists():\n",
    "    \n",
    "    fields = [\n",
    "        StructField(\"hash_key\", StringType(), True),\n",
    "        StructField(\"data_criacao_datamart\", TimestampType(), True),\n",
    "        StructField(\"dados_json\", StringType(), True),\n",
    "        StructField(\"current_timestamp\", TimestampType(), True),\n",
    "        StructField(\"status_response\", StringType(), True),\n",
    "        StructField(\"api_response_key\", StringType(), True),\n",
    "        StructField(\"qtd_envios_dia\", IntegerType(), True),\n",
    "        StructField(\"qtd_envios_total\", IntegerType(), True),\n",
    "        StructField(\"partition_key\", IntegerType(), True)\n",
    "    ]\n",
    "    \n",
    "    schema = StructType(fields)\n",
    "    dataset = sqlContext.createDataFrame(spark.sparkContext.emptyRDD(), schema)\n",
    "    \n",
    "    if not arrears_mex_table_exists(DELTA_OK_PATH): \n",
    "        dataset.write.format(TBL_FORMAT).mode(\"overwrite\").partitionBy(PARTITION).save(DELTA_OK_PATH)\n",
    "        \n",
    "    if not arrears_mex_table_exists(DELTA_REPROC_PATH): \n",
    "        dataset.write.format(TBL_FORMAT).mode(\"overwrite\").partitionBy(PARTITION).save(DELTA_REPROC_PATH)\n",
    "        \n",
    "    if not arrears_mex_table_exists(DELTA_DO_NOT_SEND_PATH): \n",
    "        dataset.write.format(TBL_FORMAT).mode(\"overwrite\").partitionBy(PARTITION).save(DELTA_DO_NOT_SEND_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrear_mex_save_to_csv():\n",
    "    '''\n",
    "        salva os dados dos tipos SUCESS e FAILURE em csv (alimentar o dashboard)\n",
    "    '''\n",
    "    dataset_sucess = arrears_mex_load_data(DELTA_OK_PATH)\n",
    "    dataset_failure = arrears_mex_load_data(DELTA_REPROC_PATH)\n",
    "    dataset_all_data = dataset_sucess.union(dataset_failure)\n",
    "    dataset_all_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(CSV_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_prevent_duplicated_data(dataset: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "        remove os registros oriundos da tabela de controle que já existem\n",
    "    '''\n",
    "    dataset_sucess = arrears_mex_load_data(DELTA_OK_PATH)\n",
    "    dataset_failure = arrears_mex_load_data(DELTA_REPROC_PATH)\n",
    "    dataset_union = dataset_sucess.union(dataset_failure)\n",
    "    dataset_result = dataset.join(dataset_union, \"hash_key\" ,\"left_anti\")\n",
    "    \n",
    "    return dataset_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_upsert_records_check_data_integrity() -> DataFrame:\n",
    "    '''\n",
    "        remove os registros da tabela de reprocessamento, se eles existem na tabela de sucesso\n",
    "        TODO: fazer um delete exists entre a tabela de falhas e a tabela de sucesso\n",
    "    '''\n",
    "    \n",
    "    dataset_sucess = arrears_mex_load_data(DELTA_OK_PATH)\n",
    "    dataset_failure = arrears_mex_load_data(DELTA_REPROC_PATH)\n",
    "    dataset = dataset_failure.join(dataset_sucess, \"hash_key\" ,\"left_anti\")\n",
    "    dataset.write.format(TBL_FORMAT).mode(\"overwrite\").partitionBy(PARTITION).save(DELTA_REPROC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_update_data(dataset: DataFrame, delta_path: str):\n",
    "    '''\n",
    "        realiza o merge dos dados\n",
    "    '''\n",
    "    delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "    delta_table\\\n",
    "        .alias(\"current_data\")\\\n",
    "        .merge(\n",
    "            dataset.alias(\"new_data\"),\n",
    "            \"current_data.hash_key = new_data.hash_key\"\n",
    "        )\\\n",
    "        .whenMatchedUpdate(\n",
    "            set = {\n",
    "                \"hash_key\": \"new_data.hash_key\", \n",
    "                \"data_criacao_datamart\": \"new_data.data_criacao_datamart\", \n",
    "                \"dados_json\": \"new_data.dados_json\", \n",
    "                \"current_timestamp\": \"new_data.current_timestamp\", \n",
    "                \"status_response\": \"new_data.status_response\", \n",
    "                \"api_response_key\": \"new_data.api_response_key\", \n",
    "                \"qtd_envios_dia\": \"new_data.qtd_envios_dia\", \n",
    "                \"qtd_envios_total\": \"new_data.qtd_envios_total\", \n",
    "                \"partition_key\": \"new_data.partition_key\"\n",
    "            }\n",
    "        )\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_data(dataset: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "      reprocessa as linhas (só atualiza se as condições forem satisfeitas)\n",
    "    '''\n",
    "    logger.info(\"update_xml_status => inicio\")\n",
    "    \n",
    "    current_date = F.current_date()\n",
    "    dataset = dataset\\\n",
    "        .withColumn(\"qtd_envios_dia\", \n",
    "                    F.when(\n",
    "                            (F.col(\"qtd_envios_dia\") == 3) & \n",
    "                            (F.to_date(F.col(\"current_timestamp\")) != F.lit(current_date)), \n",
    "                            F.lit(1)\n",
    "                          ).otherwise(F.col(\"qtd_envios_dia\"))\n",
    "                    )\\\n",
    "    \n",
    "    dataset_agg = dataset\\\n",
    "        .withColumn(\"api_response_key\", arrears_mex_process_request(F.col(\"dados_json\")))\\\n",
    "        .withColumn(\"qtd_envios_dia\", F.col(\"qtd_envios_dia\") + F.lit(1))\\\n",
    "        .withColumn(\"qtd_envios_total\", F.col(\"qtd_envios_total\") + F.lit(1))\\\n",
    "        .withColumn(\"current_timestamp\", F.lit(datetime.fromtimestamp(time.time())))\n",
    "    \n",
    "    return dataset_agg\\\n",
    "        .withColumn(\"status_response\", \n",
    "                    F.when(\n",
    "                            F.col(\"api_response_key\") != F.lit(\"\"), \n",
    "                            F.lit(\"SUCESS\")\n",
    "                          ).otherwise(F.lit(\"FAILURE\"))\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_upsert_records(dataset: DataFrame):\n",
    "    '''\n",
    "        salva automaticamente os dados nos seguintes diretórios:\n",
    "        - DELTA_OK_PATH\n",
    "        - DELTA_REPROC_PATH\n",
    "        - DELTA_DO_NOT_SEND_PATH\n",
    "    '''\n",
    "    dataset_api = process_all_data(dataset)\n",
    "    \n",
    "    # grava no diretorio de sucesso\n",
    "    dataset_sucess = dataset_api\\\n",
    "    .filter(\n",
    "        F.col(\"status_response\") == 'SUCESS'\n",
    "    )\n",
    "    \n",
    "    if dataset_sucess.count() > 0:\n",
    "        if arrears_mex_table_exists(DELTA_OK_PATH):\n",
    "            arrears_mex_update_data(dataset_sucess, DELTA_OK_PATH)\n",
    "        \n",
    "    # grava no diretorio de falha\n",
    "    dataset_failure = dataset_api\\\n",
    "    .filter(\n",
    "        (F.col(\"status_response\") == 'FAILURE') &\n",
    "        (F.col(\"qtd_envios_dia\") < 3) & \n",
    "        (F.col(\"qtd_envios_total\") < 15)\n",
    "    )\n",
    "    \n",
    "    if dataset_failure.count() > 0:\n",
    "        if arrears_mex_table_exists(DELTA_REPROC_PATH):\n",
    "            arrears_mex_update_data(dataset_failure, DELTA_REPROC_PATH)\n",
    "\n",
    "    # grava no diretorio de historiamento\n",
    "    dataset_do_not_send = dataset_api\\\n",
    "    .filter(\n",
    "        (F.col(\"status_response\") == 'FAILURE') & \n",
    "        (F.col(\"qtd_envios_total\") == 15)\n",
    "    )\n",
    "    \n",
    "    if dataset_do_not_send.count() > 0:\n",
    "        if arrears_mex_table_exists(DELTA_DO_NOT_SEND_PATH):\n",
    "            arrears_mex_update_data(dataset_do_not_send, DELTA_DO_NOT_SEND_PATH)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_transform_001(dataset: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "        tratamento inicial dos dados, retorna um dataframe\n",
    "    '''\n",
    "    logger.info(\"arrears_mex_transform_initial_dataset => inicio\")\n",
    "    \n",
    "    dataset = dataset\\\n",
    "        .withColumnRenamed(\"arrearsEventDateTime\", \"arrearsEventDate\")\\\n",
    "        .withColumnRenamed(\"levelOfApplicationId\", \"applicationId\")\\\n",
    "        .withColumnRenamed(\"levelOfApplicationCode\", \"typeCode\")\\\n",
    "        .withColumnRenamed(\"levelOfApplicationSourceSystemCode\", \"sourceSystemCode\")\\\n",
    "        .withColumnRenamed(\"arrearsCurrency\", \"currency\")\\\n",
    "        .withColumn(\"amount\", F.col(\"arrearsAmount\").cast(StringType()))\\\n",
    "        .drop(\"arrears\")\\\n",
    "        .drop(\"businessUnit\")\\\n",
    "        .drop(\"arrearsAmount\")\\\n",
    "        .drop(\"arrearsLevelOfApplication\")\n",
    "    \n",
    "    logger.info(\"arrears_mex_transform_initial_dataset => finalizado\")\n",
    "    \n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_transform_002(dataset: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "        prepara as colunas do dataframe para montar o json (com os sub-nodes)\n",
    "    '''\n",
    "    \n",
    "    logger.info(\"arrears_mex_transform_001 => inicio\")\n",
    "    \n",
    "    result = dataset\\\n",
    "        .withColumn(\"arrearsEventDate\", \n",
    "            F.unix_timestamp(\n",
    "                F.col(\"arrearsEventDate\"), TIME_FORMAT).cast('timestamp')\n",
    "        )\\\n",
    "        .withColumn(\"effectiveEventDate\", \n",
    "            F.unix_timestamp(\n",
    "                F.col(\"effectiveEventDate\"), TIME_FORMAT).cast('timestamp')\n",
    "        )\\\n",
    "        .withColumn('businessUnit', \n",
    "            F.struct(\n",
    "                F.col(\"code\"), \n",
    "                F.col(\"subCode\")\n",
    "            )\n",
    "        )\\\n",
    "        .withColumn('applicationLevel', \n",
    "            F.struct(\n",
    "                F.col(\"applicationId\"), \n",
    "                F.col(\"sourceSystemCode\"), \n",
    "                F.col(\"typeCode\")\n",
    "            )\n",
    "        )\\\n",
    "        .withColumn('arrear', \n",
    "            F.struct(\n",
    "                F.col(\"tradeId\"), \n",
    "                F.col(\"tradeSourceSystemCode\"), \n",
    "                F.col(\"facilityId\"), \n",
    "                F.col(\"facilitySourceSystemCode\"), \n",
    "                F.col(\"originalDueDate\"), \n",
    "                F.col(\"currency\"), \n",
    "                F.col(\"amount\")\n",
    "            )\n",
    "        )\\\n",
    "        .drop(\"code\")\\\n",
    "        .drop(\"subCode\")\\\n",
    "        .drop(\"applicationId\")\\\n",
    "        .drop(\"sourceSystemCode\")\\\n",
    "        .drop(\"typeCode\")\\\n",
    "        .drop(\"tradeId\")\\\n",
    "        .drop(\"tradeSourceSystemCode\")\\\n",
    "        .drop(\"facilityId\")\\\n",
    "        .drop(\"facilitySourceSystemCode\")\\\n",
    "        .drop(\"originalDueDate\")\\\n",
    "        .drop(\"currency\")\\\n",
    "        .drop(\"amount\")\n",
    "    \n",
    "    result_agg = result\\\n",
    "        .withColumnRenamed(\"arrearsEventCode\", \"code\")\\\n",
    "        .withColumnRenamed(\"arrearsEventDate\", \"eventDate\")\\\n",
    "        .withColumnRenamed(\"effectiveEventDate\", \"effectiveDate\")\\\n",
    "        .withColumnRenamed(\"suspensionReasonCode\", \"reasonCode\")\\\n",
    "        .withColumnRenamed(\"arrearsEventUserId\", \"eventUserId\")\\\n",
    "        .withColumnRenamed(\"arrearsEventSourceSystemCode\", \"eventCodeSystemSource\")\n",
    "    \n",
    "    logger.info(\"arrears_mex_transform_001 => finalizado\")\n",
    "    \n",
    "    return result_agg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_transform_003(dataset: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "        geração do json final\n",
    "    '''\n",
    "    logger.info(\"arrears_mex_transform_002 => inicio\")\n",
    "    \n",
    "    columns_json = [\n",
    "        'code',\n",
    "        'eventDate',\n",
    "        'effectiveDate',\n",
    "        'eventCodeSystemSource',\n",
    "        'eventUserId',\n",
    "        'reasonCode',\n",
    "        'businessUnit',\n",
    "        'arrear'\n",
    "    ]\n",
    "\n",
    "    result_json = dataset\\\n",
    "        .withColumn(\"dados_json\", F.to_json(F.struct(columns_json)))\\\n",
    "        .withColumn(\"status_response\", F.lit('').cast(StringType()))\\\n",
    "        .withColumn(\"api_response_key\", F.lit('').cast(StringType()))\\\n",
    "        .select (\n",
    "            F.col(\"dados_json\"),\n",
    "            F.col(\"Data_Criacao_Datamart\"), \n",
    "            F.col(\"Qtde_Envio\"),\n",
    "            F.col(\"HASH_KEY\"),\n",
    "            F.col(\"status_response\"),\n",
    "            F.col(\"api_response_key\")\n",
    "        )\n",
    "    \n",
    "    logger.info(\"arrears_mex_transform_002 => finalizado\")\n",
    "    \n",
    "    return result_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrears_mex_transform_004(dataset: DataFrame) -> DataFrame:\n",
    "    ''' \n",
    "        prepara as colunas do dataframe para montar o json (com dados novos)\n",
    "    '''\n",
    "    logger.info(\"arrears_mex_transf_process_new_data => inicio\")\n",
    "    \n",
    "    columns = [\n",
    "        'hash_key', \n",
    "        'data_criacao_datamart', \n",
    "        'dados_json', \n",
    "        'current_timestamp', \n",
    "        'status_response',\n",
    "        'api_response_key',\n",
    "        'qtd_envios_dia', \n",
    "        'qtd_envios_total',\n",
    "        'partition_key'\n",
    "    ]\n",
    "\n",
    "    dataset_agg = dataset\\\n",
    "        .withColumn(\"partition_key\", F.date_format(F.col(\"current_timestamp\"), 'yyyyMMdd').cast(IntegerType()))\\\n",
    "        .select(\n",
    "            F.col(\"HASH_KEY\"), \n",
    "            F.col(\"data_criacao_datamart\"), \n",
    "            F.col(\"dados_json\"), \n",
    "            F.col(\"current_timestamp\"),\n",
    "            F.lit(\"FAILURE\").alias(\"status_response\"),\n",
    "            F.lit(\"\").alias(\"api_response_key\"),\n",
    "            F.lit(0).alias(\"qtd_envios_dia\"),\n",
    "            F.lit(0).alias(\"qtd_envios_total\"), \n",
    "            F.col(\"partition_key\"))\\\n",
    "        .toDF(*columns)\n",
    "\n",
    "    return dataset_agg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "#      INICIO DO PROCESSAMENTO   #\n",
    "##################################\n",
    "\n",
    "\n",
    "def arrears_mex_start_pipeline(path_tabela_delta, debug: bool = False):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        logger.info(\"iniciando o processamento dos dados em: %s\" % datetime.fromtimestamp(time.time()))\n",
    "        \n",
    "        # so roda na primeira execução\n",
    "        create_enviroment_if_not_exists()\n",
    "        \n",
    "        # pega os dados antigos que devem ser reprocessados\n",
    "        dataset_old_data = arrears_mex_load_data(DELTA_REPROC_PATH)\n",
    "\n",
    "        # pega os novos dados que ainda nao foram processados\n",
    "        dataset_initial = arrears_mex_load_data(path_tabela_delta)\n",
    "        dataset_initial = arrears_mex_prevent_duplicated_data(dataset_initial)\n",
    "        if dataset_initial.count() > 1:\n",
    "            dataset_step_001 = arrears_mex_transform_001(dataset_initial)\n",
    "            dataset_step_002 = arrears_mex_transform_002(dataset_step_001)\n",
    "            dataset_step_003 = arrears_mex_transform_003(dataset_step_002)\n",
    "            dataset_new_data = arrears_mex_transform_004(dataset_step_003)\n",
    "            \n",
    "            # faz um union com os dados novos e manda reprocessar\n",
    "            dataset_union = dataset_new_data.union(dataset_old_data)\n",
    "            if dataset_union.count() > 1:\n",
    "                arrears_mex_upsert_records(dataset_union)\n",
    "                arrears_mex_upsert_records_check_data_integrity()\n",
    "                arrear_mex_save_to_csv()\n",
    "        else:\n",
    "            # reprocessa somente os dados antigos\n",
    "            if dataset_old_data.count() > 1:\n",
    "                arrears_mex_upsert_records(dataset_old_data)\n",
    "                arrears_mex_upsert_records_check_data_integrity()\n",
    "                arrear_mex_save_to_csv()\n",
    "                \n",
    "        end = time.time()\n",
    "        print(\"tempo de processamento: %s\" % str(end-start))\n",
    "        logger.info(\"finalizando o processamento dos dados em: %s\" % datetime.fromtimestamp(time.time()))\n",
    "    except Exception as e:\n",
    "        if is_debug:\n",
    "            print(str(traceback.format_exc()))\n",
    "        logger.error(\"erro ao processar os dados em: %s\" % datetime.fromtimestamp(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
