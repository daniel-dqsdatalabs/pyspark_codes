{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização de Consultas no Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Entendendo o plano de execução (Tipos de Operators e Regras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Operators:**\n",
    "\n",
    "* FileScan\n",
    "* Exchange\n",
    "* HashAggregate, SortAggregate, ObjectHashAggregate\n",
    "* SortMergeJoin\n",
    "* BroadcastHashJoin\n",
    "\n",
    "O operador FileScan representa a leitura de dados a partir de um formato qualquer de arquivo. Trabalha com 3 tipos de filtros, DataFilters, PushedFilters e PartitionFilters.\n",
    "\n",
    "\n",
    "O operador Exchange realiza o shuffle (embaralhamento) e a movimentação física (transferência) dos dados no cluster, essa movimentação de dados entre os nós é bastante onerosa e de forma geral, deve ser evitada. O exchange é induzido pelas transformações e possui as seguintes estratégias de particionamento:\n",
    "\n",
    "- Exchange HashPartitioning: \n",
    "\n",
    "\n",
    "    - groupBy\n",
    "    - distinct\n",
    "    - join\n",
    "    - repartition(\"key\")\n",
    "    - Window.partitionBy(\"key\")\n",
    "    \n",
    "    \n",
    "- Exchange SinglePartition (todos os dados serão movidos para uma única partição):\n",
    "\n",
    "\n",
    "\n",
    "    - Window.partitionBy()  \n",
    "    \n",
    "    \n",
    "- Exchange RoundRobinPartitioning (dados são movidos para um número estabelecido de partições):\n",
    "\n",
    "\n",
    "\n",
    "    - repartition(10)  \n",
    "    \n",
    "    \n",
    "- Exchange RangePartitioning:\n",
    "\n",
    "\n",
    "\n",
    "    - orderBy\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "O operador Aggregate representa uma agregação nos dados, é induzido pelas transformações groupBy, distinct e DropDuplicates e possui as seguintes estratégias (que são atribuídas em runtime).\n",
    "\n",
    "- HashAggregate     \n",
    "- SortAggregate\n",
    "- ObjectHashAggregate\n",
    "    \n",
    "\n",
    "O operador SortMergeJoin representa a junção entre dois dataframes e a ideia geral desse operador é primeiro ordenar os dados pelos valores contidos nas colunas que realizam o join para que as varreduras lineares intercaladas encontrem esses conjuntos ao mesmo tempo e, em seguida, realiza a movimentação dos dados para que cada executor mantenha uma parte específica desses dados. Como você pode imaginar, esse tipo de estratégia é onerosa: os nós precisam usar a rede para compartilhar dados e dependendo do volume, isso pode se tornar inviável.\n",
    "\n",
    "\n",
    "O operador BroadcastHashJoin também representa a junção entre dois dataframes porém utilizando outra abordagem. Durante a execução, ele se sub-divide em dois jobs com funções distintas:\n",
    "\n",
    "- BroadcastExchange\n",
    "- BroadcastHashJoin\n",
    "\n",
    "No BroadcastExchange o Spark envia uma cópia inteira de uma lookup table para cada executor, desta forma cada executor é autossuficiente na execução de operações de join. \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Rules:**\n",
    "\n",
    "- EnsureRequirements\n",
    "- ReuseExchange\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Query Execution (AQE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive Query Execution (AQE) é um dos melhores recursos do Spark 3.0, que reotimiza e ajusta os planos de consulta com base nas estatísticas de tempo de execução coletadas durante a execução da consulta. \n",
    "\n",
    "Depois de habilitar a AQE, as seguintes melhorias serão realizadas:\n",
    "\n",
    " - Conversão automatica do sort-merge join (lento) para o Broadcast join\n",
    " - Otimização do Skew Join (dados distribuidos de forma desigual entre as partições no cluster)\n",
    " - Coalescing Post-shuffle Partitions que determinam dinamicamente o número ideal de partições\n",
    "\n",
    "\n",
    "para habilitaro AQS: \n",
    "\n",
    "* spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Spark Performance Issues\")\\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\\\n",
    "    .config(\"spark.sql.parser.ansi.enabled\", \"true\")\\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .config(\"spark.metrics.conf.*.sink.console.class\", \"org.apache.spark.metrics.sink.ConsoleSink\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulando um desbalanceamento entre as partições (Data Skew)\n",
    "\n",
    "\n",
    "Data Skew é uma condição em que os dados de uma tabela são distribuídos de forma desigual entre as partições no cluster. A distorção de dados pode prejudicar gravemente o desempenho das consultas, especialmente aquelas com junções. As junções entre tabelas grandes exigem dados embaralhados (shuffling) e a distorção pode levar a um  desequilíbrio extremo de trabalho no cluster. É provável que a distorção de dados esteja afetando a performance de uma consulta caso ela pareça estar travada ao termino de poucas tasks (por exemplo, as últimas 3 tasks de 200). \n",
    "\n",
    "O Spark foi desenvolvido para funcionar com partições de tamanhos iguais e o objetivo é espalhar as partições para os executores da maneira mais uniforme possível. O particionamento de hash tenta espalhar os dados da maneira mais uniforme possível em todas as partições com base nas chaves de join.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "\n",
    "spark.range(1000000)\\\n",
    "    .withColumn(\"join_key\", F.lit(\" \"))\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"table_x\")\n",
    "\n",
    "spark.range(1000000)\\\n",
    "    .withColumn(\"join_key\", F.lit(\" \"))\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"table_y\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código a seguir, será realizado o join entre as duas tabelas, o que irá produzir um output de um trilhão de linhas, e todas elas serão produzidas utilizando um único executor (o executor que obtém a chave \"\"):\n",
    "\n",
    "Nesse processo, quando a action for executado o Spark ira computar o hash da coluna do join e realiza uma ordenação. Em seguida, ele tenta manter os registros com os mesmos hashes de ambas as partições no mesmo executor de forma que todos os valores nulos da tabela vão para um executor e o spark entra em um loop contínuo de embaralhamento e coleta de lixo, até falhar. Nesse caso, há apenas uma chave de join problemática. Em outros cenários, podem existir outros fatores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT id, count()\n",
    "    FROM\n",
    "    (\n",
    "        SELECT x.id\n",
    "        FROM table_x x\n",
    "        JOIN table_y y ON x.join_key = y.join_key\n",
    "    )\n",
    "    GROUP BY id\n",
    "\"\"\"\n",
    "\n",
    "df = sql_context.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['id], ['id, unresolvedalias('count(), None)]\n",
      "+- 'SubqueryAlias __auto_generated_subquery_name\n",
      "   +- 'Project ['x.id]\n",
      "      +- 'Join Inner, ('x.join_key = 'y.join_key)\n",
      "         :- 'SubqueryAlias x\n",
      "         :  +- 'UnresolvedRelation [table_x]\n",
      "         +- 'SubqueryAlias y\n",
      "            +- 'UnresolvedRelation [table_y]\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id: bigint, count(): bigint\n",
      "Aggregate [id#17L], [id#17L, count() AS count()#372L]\n",
      "+- SubqueryAlias __auto_generated_subquery_name\n",
      "   +- Project [id#17L]\n",
      "      +- Join Inner, (join_key#19 = join_key#338)\n",
      "         :- SubqueryAlias x\n",
      "         :  +- SubqueryAlias table_x\n",
      "         :     +- Project [id#17L,   AS join_key#19]\n",
      "         :        +- Range (0, 1000000, step=1, splits=Some(4))\n",
      "         +- SubqueryAlias y\n",
      "            +- SubqueryAlias table_y\n",
      "               +- Project [id#336L,   AS join_key#338]\n",
      "                  +- Range (0, 1000000, step=1, splits=Some(4))\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [id#17L], [id#17L, 0 AS count()#372L]\n",
      "+- Join Inner\n",
      "   :- Range (0, 1000000, step=1, splits=Some(4))\n",
      "   +- Project\n",
      "      +- Range (0, 1000000, step=1, splits=Some(4))\n",
      "\n",
      "== Physical Plan ==\n",
      "*(4) HashAggregate(keys=[id#17L], functions=[], output=[id#17L, count()#372L])\n",
      "+- Exchange hashpartitioning(id#17L, 10), true, [id=#360]\n",
      "   +- *(3) HashAggregate(keys=[id#17L], functions=[], output=[id#17L])\n",
      "      +- BroadcastNestedLoopJoin BuildRight, Inner\n",
      "         :- *(1) Range (0, 1000000, step=1, splits=4)\n",
      "         +- BroadcastExchange IdentityBroadcastMode, [id=#355]\n",
      "            +- *(2) Project\n",
      "               +- *(2) Range (0, 1000000, step=1, splits=4)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se a action for disparada o job entrará em loop infinito!!!\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Parsed Logical Plan ==\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Project [cast(id#28L as string) AS id#48, cast(count()#43L as string) AS count()#49]\n",
    "      +- Aggregate [id#28L], [id#28L, count() AS count()#43L]\n",
    "         +- SubqueryAlias __auto_generated_subquery_name\n",
    "            +- Project [id#28L]\n",
    "               +- Join Inner, (join_key#30 = join_key#37)\n",
    "                  :- SubqueryAlias x\n",
    "                  :  +- SubqueryAlias table_x\n",
    "                  :     +- Project [id#28L,   AS join_key#30]\n",
    "                  :        +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "                  +- SubqueryAlias y\n",
    "                     +- SubqueryAlias table_y\n",
    "                        +- Project [id#35L,   AS join_key#37]\n",
    "                           +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "id: string, count(): string\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Project [cast(id#28L as string) AS id#48, cast(count()#43L as string) AS count()#49]\n",
    "      +- Aggregate [id#28L], [id#28L, count() AS count()#43L]\n",
    "         +- SubqueryAlias __auto_generated_subquery_name\n",
    "            +- Project [id#28L]\n",
    "               +- Join Inner, (join_key#30 = join_key#37)\n",
    "                  :- SubqueryAlias x\n",
    "                  :  +- SubqueryAlias table_x\n",
    "                  :     +- Project [id#28L,   AS join_key#30]\n",
    "                  :        +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "                  +- SubqueryAlias y\n",
    "                     +- SubqueryAlias table_y\n",
    "                        +- Project [id#35L,   AS join_key#37]\n",
    "                           +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Aggregate [id#28L], [cast(id#28L as string) AS id#48, 0 AS count()#49]\n",
    "      +- Join Inner\n",
    "         :- Range (0, 1000000, step=1, splits=Some(4))\n",
    "         +- Project\n",
    "            +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Physical Plan ==\n",
    "CollectLimit 21\n",
    "+- *(4) HashAggregate(keys=[id#28L], functions=[], output=[id#48, count()#49])\n",
    "   +- Exchange hashpartitioning(id#28L, 10), true, [id=#57]\n",
    "      +- *(3) HashAggregate(keys=[id#28L], functions=[], output=[id#28L])\n",
    "         +- BroadcastNestedLoopJoin BuildRight, Inner\n",
    "            :- *(1) Range (0, 1000000, step=1, splits=4)\n",
    "            +- BroadcastExchange IdentityBroadcastMode, [id=#52]\n",
    "               +- *(2) Project\n",
    "                  +- *(2) Range (0, 1000000, step=1, splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos testar a nova funcionalidade do spark que promete aumentar o desempenho nessas situações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disparando a action com a config habilitada (mesmo com a otimização habilitado, entrou em loop infinito)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "== Parsed Logical Plan ==\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Project [cast(id#0L as string) AS id#16, cast(count()#11L as string) AS count()#17]\n",
    "      +- Aggregate [id#0L], [id#0L, count() AS count()#11L]\n",
    "         +- SubqueryAlias __auto_generated_subquery_name\n",
    "            +- Project [id#0L]\n",
    "               +- Join Inner, (join_key#2 = join_key#7)\n",
    "                  :- SubqueryAlias x\n",
    "                  :  +- SubqueryAlias table_x\n",
    "                  :     +- Project [id#0L,   AS join_key#2]\n",
    "                  :        +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "                  +- SubqueryAlias y\n",
    "                     +- SubqueryAlias table_y\n",
    "                        +- Project [id#5L,   AS join_key#7]\n",
    "                           +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Analyzed Logical Plan ==\n",
    "id: string, count(): string\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Project [cast(id#0L as string) AS id#16, cast(count()#11L as string) AS count()#17]\n",
    "      +- Aggregate [id#0L], [id#0L, count() AS count()#11L]\n",
    "         +- SubqueryAlias __auto_generated_subquery_name\n",
    "            +- Project [id#0L]\n",
    "               +- Join Inner, (join_key#2 = join_key#7)\n",
    "                  :- SubqueryAlias x\n",
    "                  :  +- SubqueryAlias table_x\n",
    "                  :     +- Project [id#0L,   AS join_key#2]\n",
    "                  :        +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "                  +- SubqueryAlias y\n",
    "                     +- SubqueryAlias table_y\n",
    "                        +- Project [id#5L,   AS join_key#7]\n",
    "                           +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Optimized Logical Plan ==\n",
    "GlobalLimit 21\n",
    "+- LocalLimit 21\n",
    "   +- Aggregate [id#0L], [cast(id#0L as string) AS id#16, 0 AS count()#17]\n",
    "      +- Join Inner\n",
    "         :- Range (0, 1000000, step=1, splits=Some(4))\n",
    "         +- Project\n",
    "            +- Range (0, 1000000, step=1, splits=Some(4))\n",
    "\n",
    "== Physical Plan ==\n",
    "AdaptiveSparkPlan isFinalPlan=false\n",
    "+- CollectLimit 21\n",
    "   +- HashAggregate(keys=[id#0L], functions=[], output=[id#16, count()#17])\n",
    "      +- ShuffleQueryStage 1\n",
    "         +- Exchange hashpartitioning(id#0L, 10), true, [id=#85]\n",
    "            +- *(3) HashAggregate(keys=[id#0L], functions=[], output=[id#0L])\n",
    "               +- BroadcastNestedLoopJoin BuildRight, Inner\n",
    "                  :- *(2) Range (0, 1000000, step=1, splits=4)\n",
    "                  +- BroadcastQueryStage 0\n",
    "                     +- BroadcastExchange IdentityBroadcastMode, [id=#49]\n",
    "                        +- *(1) Project\n",
    "                           +- *(1) Range (0, 1000000, step=1, splits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tentativa de otimização utilizando uma outra abordagem:\n",
    "\n",
    "Vamos dividir a tabela em duas partes. A primeira parte conterá todas as linhas que não têm uma chave nula e a segunda parte conterá todos os dados sem valores nulos.\n",
    "\n",
    "Em seguida, iremos alterar / reescrever nossa lógica ETL, onde podemos realizar um join à esquerda com a tabela not_null e simplesmente executar uma união com a coluna nula, pois as chaves nulas não participarão do join. Portanto, ao seguir essa técnica, podemos evitar um embaralhamento e o problema de pausa GC na tabela com grandes valores nulos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|count(0)|join_key|\n",
      "+--------+--------+\n",
      "| 1000000|        |\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql_context.sql(\"SELECT * FROM table_x WHERE join_key IS NULL\").createOrReplaceTempView(\"table_x_id_null\")\n",
    "sql_context.sql(\"SELECT * FROM table_x WHERE join_key IS NOT NULL\").createOrReplaceTempView(\"table_x_id_not_null\")\n",
    "\n",
    "sql_context.sql(\"SELECT * FROM table_y WHERE join_key IS NULL\").createOrReplaceTempView(\"table_y_id_null\")\n",
    "sql_context.sql(\"SELECT * FROM table_y WHERE join_key IS NOT NULL\").createOrReplaceTempView(\"table_y_id_not_null\")\n",
    "\n",
    "# reescrever a query\n",
    "\n",
    "df = sql_context.sql(\"select COUNT(0), join_key from table_y GROUP BY join_key\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisa a distribuição dos dados nas das partições\n",
    "for i, part in enumerate(df.rdd.glom().collect()):\n",
    "    print({i: part})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
